import os
import sys

import mlflow
import streamlit as st
from langchain_openai import ChatOpenAI

# ãƒ­ãƒ¼ã‚«ãƒ«ã®PyTorchã‚’ç„¡åŠ¹åŒ–
# ã“ã‚Œã‚’ã—ãªã„ã¨ `RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_` ã¿ãŸã„ãªã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
# streamlitãŒã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ç›£è¦–ã™ã‚‹æ™‚ã«èµ·ãã‚‹ã‚¨ãƒ©ãƒ¼ã‚‰ã—ãã€æ¤œç´¢å¯¾è±¡ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã—ã¾ãˆã°è‰¯ã„
sys.modules["torch.classes"] = None

# mlflow_rag.pyã‹ã‚‰RetrievalQAä½œæˆé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from mlflow_rag import create_retrieval_qa

st.title("ğŸ¦œğŸ”— RAG Quickstart App")

# Set the active experiment
mlflow.set_experiment("Legal RAG")


# RetrievalQAãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ä¸€åº¦ã ã‘å®Ÿè¡Œã™ã‚‹ï¼‰
@st.cache_resource
def initialize_retrieval_qa():
    """RetrievalQAãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–ã™ã‚‹"""
    return create_retrieval_qa()


# RetrievalQAãƒã‚§ãƒ¼ãƒ³ã‚’å–å¾—
retrieval_qa = initialize_retrieval_qa()


@mlflow.trace(
    name="rag_llm_call",
    attributes={"model": "gemma-3-12b", "source": "local", "type": "RAG"},
)
def generate_response_with_rag(input_text):
    # RAGã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
    # RetrievalQAãƒã‚§ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
    result = retrieval_qa.invoke({"query": input_text})
    output = result["result"]

    # å‚ç…§ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if "source_documents" in result:
        with st.expander("å‚ç…§ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"):
            for i, doc in enumerate(result["source_documents"]):
                st.write(f"**ã‚½ãƒ¼ã‚¹ {i+1}:**")
                st.write(
                    doc.page_content[:500] + "..."
                    if len(doc.page_content) > 500
                    else doc.page_content
                )

    st.info(output)
    return output


with st.form("my_form"):
    text = st.text_area(
        "Enter text:",
        "What does the document say about trespassers?",
    )
    submitted = st.form_submit_button("Submit")
    if submitted:
        generate_response_with_rag(text)
